{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 常用爬虫框架scrapy和pyspider\n",
    "- scrapy\n",
    "    - 高性能数据解析操作，持久化存储操作，高性能数据下载操作...\n",
    "    - 环境安装（Windows）：\n",
    "        - a. pip install wheel\n",
    "        - b. 下载twisted：http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted\n",
    "        - c. 进入下载目录：执行 pip install Twisted-17.1.0-cp35m-win_amd64.whl\n",
    "        - d. pip install pywin32\n",
    "        - e. pip install scrapy\n",
    "    - 基本使用\n",
    "        - 创建一个工程：scrapy startproject 工程名称\n",
    "        - 必须在spiders目录下，创建一个爬虫文件\n",
    "            - 1. cd 工程名称\n",
    "            - 2. scrapy genspider 爬虫文件名称 url\n",
    "            - 3. 执行工程：srcapy crawl 爬虫文件名称；srcapy crawl 爬虫文件名称 --nolog（表明无日志信息，建议不使用）\n",
    "        - settings.py\n",
    "            - 不遵从roboots.txt协议：ROBOTSTXT_OBEY = False\n",
    "            - UA伪装：USER_AGENT = 'XXX'\n",
    "            - 输出日志，只输出错误信息：LOG_LEVEL = 'ERROR'\n",
    "    - 持久化存储\n",
    "        - 基于终端指令\n",
    "            - 特性：只可以将parse()方法的返回值存储到本地磁盘文件中\n",
    "            - 指令：scrapy crawl 爬虫文件名称 -o 存储文件名称\n",
    "        - 基于管道\n",
    "            - 1. 数据解析\n",
    "            - 2. 在items.py文件中，定义相关属性\n",
    "            - 3. 将解析后的数据存储或封装到item对象中（items.py文件中对应类的对象）\n",
    "            - 4. 向管道提交item对象：yield item\n",
    "            - 5. 在管道文件的process_item()方法中，进行持久化存储\n",
    "            - 6. 在配置文件settings.py中开启管道：ITEM_PIPELINES = {XXX}\n",
    "    \n",
    "    - 如何将同一份数据存储到不同平台？\n",
    "        - 分析：\n",
    "            - 1. 管道文件pipelines.py中一个管道类负责数据的一种形式的持久化存储\n",
    "            - 2. 爬虫文件向管道提交item，只会提交给优先级最高的管道类\n",
    "            - 3. 在管道类的process_item()方法中，return item：表示将当前管道类接收的item，传递给下一个即将执行的管道类\n",
    "    \n",
    "    - 在scrapy中如何进行手动改请求发送？\n",
    "        - 使用场景：爬取多个页码对应的页面源码数据\n",
    "        - 手动请求发送\n",
    "            - 注释：scrapy.Request(url,callback,method='GET')，默认GET方式发送请求，scrapy中不发送post请求\n",
    "            - yield scrapy.Request(newUrl,callback=self.parse)\n",
    "\n",
    "    - scrapy五大核心组件工作流程\n",
    "        - 引擎(Scrapy)：对整个系统的数据流进行处理, 触发事务(框架核心).\n",
    "\n",
    "        - 调度器(Scheduler)：用来接受引擎发过来的请求. 由过滤器过滤重复的url并将其压入队列中, 在引擎再次请求的时候返回. 可以想像                       成一个URL（抓取网页的网址或者说是链接）的优先队列, 由它来决定下一个要抓取的网址是什么.\n",
    "\n",
    "        - 下载器(Downloader)：用于下载网页内容, 并将网页内容返回给蜘蛛(Scrapy下载器是建立在twisted这个高效的异步模型上的).\n",
    "\n",
    "        - 爬虫(Spiders)：爬虫是主要干活的, 它可以生成url, 并从特定的url中提取自己需要的信息, 即所谓的实体(Item). 用户也可以从中                     提取出链接, 让Scrapy继续抓取下一个页面.\n",
    "\n",
    "        - 管道(Pipeline)：负责处理爬虫从网页中抽取的实体, 主要的功能是持久化实体、验证实体的有效性、清除不需要的信息. 当页面被爬                     虫解析后, 将被发送到项目管道, 并经过几个特定的次序处理数据.\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
