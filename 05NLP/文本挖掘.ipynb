{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是文本挖掘？\n",
    "- 数据不光是0-9这样的数字，数据是信息的载体\n",
    "- 非数字类的数据难以做到定量化；总信息量过大，人工难以承受；次要信息难以压缩，有效信息难以提取；难以纳入数学分析框架\n",
    "- 文本挖掘意义：\n",
    "- NLP流程：\n",
    "    - 1. 原始文本：网页文本、新闻、报告\n",
    "    - 2. 分词：英文->空格，中文->无\n",
    "    - 3. 清洗：无用标签、特殊符号、停用词、大写转小写\n",
    "    - 4. 标准化：go,going,went->go\n",
    "    - 5. 特征提取：string->向量\n",
    "    - 6. 建模：相似度算法、分类算法等（机器学习算法）\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词\n",
    "- 分词工具：Jieba分词、SnowNLP、LTP、HanNLP等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "征战/四海/只/为/今日/一胜/，/我/不会/再败/了/。\n",
      "['征战', '四海', '只', '为', '今日', '一胜', '，', '我', '不会', '再败', '了', '。']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "text = \"征战四海只为今日一胜，我不会再败了。\"\n",
    "# cut(sentence,cut_all)：返回：generator类型\n",
    "seg = jieba.cut(text)\n",
    "print(\"/\".join(seg))\n",
    "\n",
    "# lcut()：返回list类型\n",
    "seg = jieba.lcut(text)\n",
    "print(seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 词性分析\n",
    "    - posseg.cut(text)：返回generator\n",
    "    - posseg.lcut(text)：返回list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pair('征战', 'v'), pair('四海', 'ns'), pair('只', 'd'), pair('为', 'p'), pair('今日', 't'), pair('一', 'm'), pair('胜', 'v'), pair('，', 'x'), pair('我', 'r'), pair('不会', 'v'), pair('再败', 'v'), pair('了', 'ul'), pair('。', 'x')]\n",
      "[pair('征战', 'v'), pair('四海', 'ns'), pair('只', 'd'), pair('为', 'p'), pair('今日', 't'), pair('一', 'm'), pair('胜', 'v'), pair('，', 'x'), pair('我', 'r'), pair('不会', 'v'), pair('再败', 'v'), pair('了', 'ul'), pair('。', 'x')]\n"
     ]
    }
   ],
   "source": [
    "import jieba.posseg as posseg\n",
    "\n",
    "text = \"征战四海只为今日一胜，我不会再败了。\"\n",
    "seg = posseg.cut(text)\n",
    "print([se for se in seg])\n",
    "\n",
    "# 直接返回列表形式\n",
    "seg = posseg.lcut(text)\n",
    "print(seg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 关键词抽取\n",
    "    - 基于TF-IDF算法\n",
    "    - 基于TextRank算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['一胜', '再败', '征战']\n",
      "['征战', '再败', '四海']\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse as analyse\n",
    "\n",
    "text = \"征战四海只为今日一胜，我不会再败了。\"\n",
    "# TF-IDF算法\n",
    "rf_result = analyse.extract_tags(text,topK=3)   # topK：指关键词数量，默认20\n",
    "print(rf_result)\n",
    "\n",
    "# TextRank\n",
    "tr_result = analyse.textrank(text,topK=3)   # topK：指关键词数量，默认20\n",
    "print(tr_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 分词：\n",
    "    - 精确模式：最常用的分词方法\n",
    "    - 全模式：所有可能的词都列举出来\n",
    "    - 搜索引擎模式：适用于搜索引擎使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天天气 真 好\n",
      "今天 今天天气 天天 天气 真好\n",
      "今天 天天 天气 今天天气 真 好\n",
      "['今天天气', '真', '好']\n",
      "['今天', '今天天气', '天天', '天气', '真好']\n",
      "['今天', '天天', '天气', '今天天气', '真', '好']\n"
     ]
    }
   ],
   "source": [
    "text = \"今天天气真好\"\n",
    "# 精确模式\n",
    "accurateModel = jieba.cut(text,cut_all=False)\n",
    "# 全模式\n",
    "fullModel = jieba.cut(text,cut_all=True)\n",
    "# 搜索引擎模式\n",
    "searchModel = jieba.cut_for_search(text)\n",
    "print(' '.join(accurateModel))\n",
    "print(' '.join(fullModel))\n",
    "print(' '.join(searchModel))\n",
    "\n",
    "\n",
    "# 直接返回list\n",
    "accurateModel = jieba.lcut(text,cut_all=False)\n",
    "# 全模式\n",
    "fullModel = jieba.lcut(text,cut_all=True)\n",
    "# 搜索引擎模式\n",
    "searchModel = jieba.lcut_for_search(text)\n",
    "print(accurateModel)\n",
    "print(fullModel)\n",
    "print(searchModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 增加词语\n",
    "    - 自定义字典：load_userdict(file_name)，一个词占一行；每一行分三部分：词语、词频（可省略）、词性（可省略），用空格隔开\n",
    "    - 增加词语：add_word(word)\n",
    "    - 删除词语：del_word(word) \n",
    "    - 调节单个词语的词频：suggest_freq(segment,tune=True)：调节单个词语的词频，使其不能被分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['李编辑', '如果', '不去', '北京', '，', '那么', '他', '就', '会', '不喜欢', '也', '不高兴']\n"
     ]
    }
   ],
   "source": [
    "jieba.add_word(\"李编辑\") # 在程序中动态修改词典\n",
    "jieba.suggest_freq(\"不喜欢\",tune=True)\n",
    "jieba.suggest_freq(\"不高兴\",tune=True)\n",
    "\n",
    "text = \"李编辑如果不去北京，那么他就会不喜欢也不高兴\"\n",
    "seg = jieba.lcut(text)\n",
    "print(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 拼写纠错\n",
    "- 用户输入->候选->编辑距离\n",
    "    - 之前方法：用户输入->从词典中训导编辑距离最小的->返回\n",
    "    - 现在方法：用户输入->生成编辑距离为1，2的字符串->过滤->返回\n",
    "- 如何过滤？\n",
    "\n",
    "- 停用词过滤、出现频率低的词汇过滤\n",
    "    - Stemming、Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本表示\n",
    "词典：[我们，去，爬山，今天，你们，昨天，跑步]\n",
    " \n",
    "每个单词的表示：<font color='red'>one-hot方法，不统计频率，只要出现则为1</font>\n",
    " \n",
    "我们：(1,0,0,0,0,0,0) -> 7维=|词典|\n",
    "\n",
    "爬山：(0,0,1,0,0,0,0) -> 7维=|词典|\n",
    "\n",
    "跑步：(0,0,0,0,0,0,1) -> 7维=|词典|\n",
    "\n",
    "昨天：(0,0,0,0,0,1,0) -> 7维=|词典|\n",
    "\n",
    "词典：[我们，又，去，爬山，今天，你们，昨天，跑步]\n",
    "\n",
    "每个句子的表示：<font color='red'>Boolean方法，不统计频率，只要出现则为1</font>\n",
    "\n",
    "我们 今天 去 爬山：(1,0,1,1,1,0,0,0) -> 8维=|词典|\n",
    "\n",
    "你们 昨天 跑步：(0,0,0,0,0,1,1,1) -> 8维=|词典|\n",
    "\n",
    "你们 又 去 爬山 又 去 跑步：(0,1,1,1,0,1,0,1) -> 8维=|词典|\n",
    "\n",
    "词典：[我们，又，去，爬山，今天，你们，昨天，跑步]\n",
    "\n",
    "每个句子的表示：<font color='red'>count-based方法，统计频率</font>\n",
    "\n",
    "我们 今天 去 爬山：(1,0,1,1,1,0,0,0) -> 8维=|词典|\n",
    ",\n",
    "你们 昨天 跑步：(0,0,0,0,0,1,1,1) -> 8维=|词典|\n",
    "\n",
    "你们 又 去 爬山 又 去 跑步：(0,2,2,1,0,1,0,1) -> 8维=|词典|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 文本相似度\n",
    "<font color='red'>计算距离（欧式距离）：d = |s1 - s2|。距离越大，个体差异越大</font>\n",
    "\n",
    "s1：我们 今天 去 爬山：(1,0,1,1,1,0,0,0)\n",
    "\n",
    "s2：你们 昨天 跑步：(0,0,0,0,0,1,1,1)\n",
    "\n",
    "s3：你们 又 去 爬山 又 去 跑步：(0,2,2,1,0,1,0,1)\n",
    "\n",
    "d(s1,s2) = 根号(1²+1²+1²+1²+1²+1²+1²) = 根号(7)\n",
    "\n",
    "d(s1,s3) = 根号(1²+2²+1²+1²+1²+1²) = 根号(9)\n",
    "\n",
    "d(s2,s3) = 根号(2²+2²+1²+1²) = 根号(10)\n",
    "\n",
    "结论：sim(s1,s2)>sim(s1,s3)>sim(s2,s3)\n",
    "\n",
    "<font color='red'>计算相似度（余弦相似度）：d = (s1·s2)/(|s1|·|s2|)</font>\n",
    "\n",
    "s1：我们 今天 去 爬山：(1,0,1,1,1,0,0,0)\n",
    "\n",
    "s2：你们 昨天 跑步：(0,0,0,0,0,1,1,1)\n",
    "\n",
    "s3：你们 又 去 爬山 又 去 跑步：(0,2,2,1,0,1,0,1)\n",
    "\n",
    "d(s1,s2) = (0)/() = 0\n",
    "\n",
    "d(s1,s3) = (3)/(2*根号(11)) =3/2根号(11)\n",
    "\n",
    "d(s2,s3) = (2)/(根号(3)*根号(11)) =2/根号(33)\n",
    "\n",
    "结论：sim(s1,s3)>sim(s2,s3)>sim(s1,s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf 文本表示\n",
    "- <font color='red'> tfidf(w) = tf(d,w) * idf(w) </font>\n",
    "    - tf(d,w)：文档d中w的词频\n",
    "    - idf(w)：考虑单词的重要性，log(N/N(w))\n",
    "    - N：语料库中的文档总数\n",
    "    - N(w)：词语w出现在多少个文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词典：[今天，上，NLP，课程，的，有，意思，数据，也]\n",
    "\n",
    "|词典| = 9\n",
    "\n",
    "句子1：今天 上 NLP 课程\n",
    "\n",
    "(1·log(3/2),1·log(3/1),1·log(3/1),1·log(3/3),0,0,0,0,0) = (log(3/2),log3,log3,0,0,0,0,0,0) <font color='red'> -> tfidf向量 </font>\n",
    "\n",
    "句子2：今天 的 课程 有 意思\n",
    "\n",
    "(1·log(3/2),0,0,1·log(3/3),1·log(3/1),1·log(3/2),1·log(3/2),0,0) = (log(3/2),0,0,0,log3,log(3/2),log(3/2),0,0)\n",
    "\n",
    "句子3：数据 课程 也 有 意思\n",
    "\n",
    "(0,0,0,1·log(3/3),0,1·log(3/2),1·log(3/2),1·log(3/1),1·log(3/1)) = (0,0,0,0,0,log(3/2),log(3/2),log3,log3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 词向量\n",
    "<font color='red'> 利用Ont-hot表示法表达单词之间相似度？NO </font>\n",
    "\n",
    "<font color='red'> Distributed Representation：分布式表示方法 </font>\n",
    "\n",
    "分布式表示法的长度是人工定义\n",
    "\n",
    "我们：[0.1,0.2,0.4,0.2]\n",
    "\n",
    "爬山：[0.2,0.3,0.7,0.1]\n",
    "\n",
    "运动：[0.2,0.3,0.6,0.2]\n",
    "\n",
    "昨天：[0.5,0.9,0.1,0.3]\n",
    "\n",
    "欧氏距离：\n",
    "\n",
    "d（我们，爬山）= 根号（0.1²+0.1²+0.3²+0.1²）= 根号（0.12）\n",
    "\n",
    "d（运动，爬山）= 根号（0.02）\n",
    "\n",
    "d（运动，爬山）< d（我们，爬山） => sim（运动，爬山）> sim（我们，爬山）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习词向量\n",
    "\n",
    "<font color='red'> 从词向量到句子向量 </font>\n",
    "\n",
    "例如：\n",
    "\n",
    "我们 = （0.1，0.2，0.1，0.3）\n",
    "\n",
    "去 = （0.3，0.2，0.15，0.2）\n",
    "\n",
    "运动 = （0.2，0.15，0.4，0.7）\n",
    "\n",
    "“我们去运动” 怎么获得句子向量？\n",
    "\n",
    "方法：Average法则（最简单的方法）\n",
    "\n",
    "对3个词向量进行叠加，再进行平均，得到“我们去运动”的句子向量 = （0.2，0.18，0.22，0.4）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 倒排表（Inverted Index）\n",
    "- 在搜索引擎中有每个文件都对应一个文件ID，文件内容被表示为一系列关键词的集合（实际上在搜索引擎索引库中，关键词也已经转换为关键词ID）。例如“文档1”经过分词，提取了20个关键词，每个关键词都会记录它在文档中的出现次数和出现位置。得到正向索引的结构如下：\n",
    "\n",
    "    - 1、“文档1”的ID > 单词1：出现次数，出现位置列表；单词2：出现次数，出现位置列表；…………。\n",
    "\n",
    "    - 2、“文档2”的ID > 此文档出现的关键词列表。\n",
    "\n",
    "Doc1:我们 今天 运动\n",
    "\n",
    "Doc2:我们 昨天 运动\n",
    "\n",
    "Doc3:我们 上 课\n",
    "\n",
    "Doc4:我们 上 什么 课\n",
    "\n",
    "词典：[我们，今天，运动，昨天，上，课，什么]\n",
    "\n",
    "我们：[Doc1,Doc2]\n",
    "\n",
    "今天：[Doc1]\n",
    "\n",
    "运动：[Doc1,Doc2]\n",
    "\n",
    "昨天：[Doc2]\n",
    "\n",
    "上：[Doc3,Doc4]\n",
    "\n",
    "课：[Doc3,Doc4]\n",
    "\n",
    "什么：[Doc4]\n",
    "\n",
    "假设通过百度查询：运动，那么返回Doc1和Doc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noisy Channel Model\n",
    "\n",
    "p(text|source) = p(source|text)·p(text)/p(source) （贝叶斯公式）p(source)是一个常数，那么 p(text|source) ∝ p(source|text)·p(text)\n",
    "\n",
    "- 机器翻译\n",
    "    - eg：英->中，p(中|英) ∝ p(英|中)·p(中)，p(英|中)是翻译模型，p(中)是语言模型\n",
    "\n",
    "- 语言识别\n",
    "    - p(文本|语言信息) ∝ p(语言信息|文本)·p(文本)，p(语言信息|文本)是翻译识别模型，p(文本)是语言模型\n",
    "\n",
    "- 密码破解\n",
    "    - p(明文|暗文) ∝ p(暗文|明文)·p(明文)，p(明文)是语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 语言模型\n",
    "- 作用：用来判断是否一句话在语法上通顺\n",
    "\n",
    "比较：今天是周日 VS 今天周日是\n",
    "\n",
    "- 目标：Compute the probability of a sentence or sequence of words.\n",
    "\n",
    "#### Chain Rule\n",
    "\n",
    "条件概率：p(AB) = p(A|B)·p(B) = p(B|A)·p(A)\n",
    "\n",
    "P(ABCD) = P(A)·P(A|B)·P(C|AB)·P(D|ABC)\n",
    "\n",
    "P(今天,是,春节,我们,都,休息) = P(今天)·P(是|今天)·P(春节|今天,是)·P(我们|今天,是,春节)·P(都|今天,是,春节,我们)·P(休息|今天,是,春节,我们,休息)\n",
    "\n",
    "#### Markov假设\n",
    "\n",
    "1st order Markov assumption：P(休息|今天,是,春节,我们,都) ≈ P(休息|都)\n",
    "\n",
    "2st order Markov assumption：P(休息|今天,是,春节,我们,都) ≈ P(休息|我们,都)\n",
    "\n",
    "3st order Markov assumption：P(休息|今天,是,春节,我们,都) ≈ P(休息|春节,我们,都)\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 语言模型\n",
    "\n",
    "- Unigram\n",
    "\n",
    "P(ABCD) = P(A)·P(B)·P(C)·P(D)，变量之间相互独立\n",
    "\n",
    "P(今天,是,春节,我们,都,休息) = P(今天)·P(是)·P(春节)·P(我们)·P(都)·P(休息)\n",
    "\n",
    "P(今天,春节,是,都,我们,休息) = P(今天)·P(春节)·P(是)·P(都)·P(我们)·P(休息)\n",
    "\n",
    "- Bigram，基于1st order Markov assumption\n",
    "\n",
    "P(今天,是,春节,我们,都,休息) = P(今天)·P(是|今天)·P(春节|是)·P(我们|春节)·P(都|我们)·P(休息|都)\n",
    "\n",
    "P(今天,春节,是,都,我们,休息) = P(今天)·P(春节|今天)·P(是|春节)·P(都|是)·P(我们|都)·P(休息|我们)\n",
    "\n",
    "- N-gram\n",
    "\n",
    "N≥3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 估计语言模型的概率\n",
    "\n",
    "- Unigram：Estimating Probability\n",
    "\n",
    "语料库：\n",
    "\n",
    "今天 的 天气 很好 啊\n",
    "\n",
    "我 很 想 出去 运动\n",
    "\n",
    "但 今天 上午 有 课程\n",
    "\n",
    "训练营 明天 才 开始\n",
    "\n",
    "V = 18，去除重复的单词\n",
    "\n",
    "P(今天 开始 训练营 课程) = P(今天)·P(开始)·P(训练营)·P(课程) = 2/18·1/18·1/18·1/18 = 2/104976\n",
    "\n",
    "P(今天 没有 训练营 课程) = P(今天)·P(没有)·P(训练营)·P(课程) = 2/18·0/18·1/18·1/18 = 0\n",
    "\n",
    "- Bigram：Estimating Probability，Bigram基于1st order Markov assumption\n",
    "\n",
    "语料库：\n",
    "\n",
    "今天 的 天气 很好 啊\n",
    "\n",
    "我 很 想 出去 运动\n",
    "\n",
    "但 今天 上午 有 课程\n",
    "\n",
    "训练营 明天 才 开始\n",
    "\n",
    "V = 18，去除重复的单词\n",
    "\n",
    "P(今天 上午 想 出去 运动) = P(今天)·P(上午|今天)·P(想|上午)·P(出去|想)·P(运动|出去) = 2/18·1/2·1·1/2·1 = 1/38\n",
    "\n",
    "P(今天 上午 的 天气 很好 呢) = P(今天)·P(上午|今天)·P(的|上午)·P(天气|的)·P(很好|天气)·P(呢|很好) = 0\n",
    "\n",
    "- N-gram：Estimating Probability\n",
    "\n",
    "N = 3 时，\n",
    "\n",
    "语料库：\n",
    "\n",
    "今天 上午 的 天气 很好 啊\n",
    "\n",
    "我 很 想 出去 运动\n",
    "\n",
    "但 今天 上午 有 课程\n",
    "\n",
    "训练营 明天 才 开始\n",
    "\n",
    "V = 18，去除重复的单词\n",
    "\n",
    "P(今天 上午 有 课程) = P(今天)·P(上午|今天)·P(有|今天,上午)·P(课程|上午,有) = 2/18·1/2·1/2·1 = 1/40\n",
    "\n",
    "P(今天 没有 训练营 课程) = P(今天)·P(没有|今天)·P(训练营|今天,没有)·P(课程|没有,训练营) = 2/18·0·... = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 评估语言模型\n",
    "\n",
    "Q：训练出来的语言模型效果好还是坏？\n",
    "\n",
    "  理想情况下：\n",
    "  \n",
    "  1、假设有两个语言模型A，B\n",
    "  \n",
    "  2、选定一个特定的任务比如乒协纠错\n",
    "\n",
    "  3、把两个模型A，B都应用在此任务中\n",
    "  \n",
    "  4、最后比较准确率，从而判断A，B的表现\n",
    "\n",
    "<font color='red'>Perplexity = 2⁻ˣ，x:average log likelihood（平均对数似然）</font>\n",
    "\n",
    "<font color='red'>Perplexity越小越好</font>\n",
    "\n",
    "训练好的Bigram：\n",
    "\n",
    "P(天气|今天) = 0.01\n",
    "\n",
    "P(今天) = 0.002\n",
    "\n",
    "P(很好|天气) = 0.1\n",
    "\n",
    "P(适合|很好) = 0.01\n",
    "\n",
    "P(出去|适合) = 0.02\n",
    "\n",
    "P(运动|出去) = 0.1\n",
    "\n",
    "\n",
    "今天：P(今天) = 0.002（计算出来为likelihodd） -> log P(今天) = a1（转换成 log likelihood）\n",
    "\n",
    "今天天气：P(天气|今天) = 0.01（计算出来为likelihodd） -> log P(气|今天) = a2（转换成 log likelihood）\n",
    "\n",
    "今天天气很好：P(很好|天气) = 0.1（计算出来为likelihodd） -> log P(很好|天气) = a3（转换成 log likelihood）\n",
    "\n",
    "今天天气很好，适合：P(适合|很好) = 0.01（计算出来为likelihodd） -> log P(适合|很好) = a4（转换成 log likelihood）\n",
    "\n",
    "今天天气很好，适合出去：P(出去|适合) = 0.02（计算出来为likelihodd） -> log P(出去|适合) = a5（转换成 log likelihood）\n",
    "\n",
    "今天天气很好，适合出去运动：P(运动|出去) = 0.1（计算出来为likelihodd） -> log P(运动|出去) = a6（转换成 log likelihood）\n",
    "\n",
    "计算average log likelihood，即 x = (a1+a2+a3+a4+a5+a6)/6\n",
    "\n",
    "最后计算Perplexity = 2⁻ˣ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Smoothing：平滑方法：处理概率为0\n",
    "\n",
    "语料库：Bigram\n",
    "\n",
    "今天 上午 的 天气 很好 啊\n",
    "\n",
    "我 很 想 出去 运动\n",
    "\n",
    "但 今天 上午 有 课程\n",
    "\n",
    "训练营 明天 才 开始\n",
    "\n",
    "V = 18,去除重复的单词\n",
    "\n",
    "P(今天 训练营 没有) = P(今天)·P(训练营|今天)·P(没有|训练营) = 2/18·0·... = 0\n",
    "\n",
    "P(今天 没有 训练营 课程) = 2/18·0·... = 0\n",
    "\n",
    "如何解决上述概率为0情况？答：Add-one Smoothing、Add-K Smoothing、Interpolation、Good-Turing Smoothing\n",
    "\n",
    "<font color='red'>Add-one Smoothing</font>\n",
    "\n",
    "Pᴍʟᴇ(wᵢ|wᵢ₋₁) = c(wᵢ₋₁,wᵢ)/c(wᵢ₋₁)，存在缺陷，若分母为0，则P=0\n",
    "\n",
    "改进：Pᴍʟᴇ(wᵢ|wᵢ₋₁) = ( c(wᵢ₋₁,wᵢ) + 1 )/( c(wᵢ₋₁) + V )，V是词典大小（去除重复单词）\n",
    "\n",
    "<font color='red'>Add-K Smoothing</font>\n",
    "\n",
    "Pᴍʟᴇ(wᵢ|wᵢ₋₁) = ( c(wᵢ₋₁,wᵢ) + k )/( c(wᵢ₋₁) + kV )\n",
    "\n",
    "K怎么选择？1、k=1,2,3,...,100；2、优化\n",
    "\n",
    "<font color='red'>Interpolation</font>\n",
    "\n",
    "核心思路：在计算Trigram概率时，同时考虑Unigram,Bigram,Trigram出现的频次。\n",
    "\n",
    "P(wₙ|wₙ₋₁,wₙ₋₂) = λ₁·P(wₙ|wₙ₋₁,wₙ₋₂)+λ₂·P(wₙ|wₙ₋₁)+λ₃·P(wₙ)，λ₁+λ₂+λ₃ = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
